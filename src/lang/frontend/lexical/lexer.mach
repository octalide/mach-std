use        std.types.bool;
use        std.types.list;
use        std.types.option;
use        std.types.result;
use        std.types.size;
use        std.types.string;
use ascii: std.text.ascii;
use mem:   std.system.memory;

use std.lang.frontend.lexical.tokens;

rec DigitScanResult {
	consumed: bool;
	invalid:  bool;
}

pub rec Lexer {
	module_id: str;

	src: str;
    pos: usize;
    row: usize;
    col: usize;
}

fun make_eof_token(module_id: str, pos: usize, row: usize, col: usize) Token {
	var token: Token;
	token.kind      = TOKEN_EOF;
	token.module_id = module_id;
	token.pos       = pos;
	token.len       = 0;
	token.row       = row;
	token.col       = col;

	ret token;
}

pub fun lexer_make(source: str, module_id: str) Lexer {
	var lexer: Lexer;
	lexer.module_id = module_id;
	lexer.src       = source;
	lexer.pos       = 0;
	lexer.row       = 1;
	lexer.col       = 1;

	ret lexer;
}

pub fun (this: *Lexer) collect() List[Token] {
	var tokens:          List[Token];
	val opt_tokens_init: Option[str] = tokens.init(128);
	if (opt_tokens_init.is_some()) {
		ret tokens;
	}

	# check that source is valid
	if (this.src.data == nil || this.src.len == 0) {
		ret tokens;
	}

	# collect all tokens from source
	for (this.at_end() == false) {
		val token:    Token       = this.next_token();
		val res_push: Option[str] = tokens.push(token);
		if (res_push.is_some()) {
			ret tokens;
		}
	}

	ret tokens;
}

fun (this: *Lexer) at_end() bool {
	ret this.pos >= this.src.len;
}

fun (this: *Lexer) current() char {
	if (this.src.data == nil || this.at_end()) {
		ret '\0';
	}

	ret @(this.src.data + this.pos);
}

fun (this: *Lexer) peek(amount: usize) char {
	val pos: usize = this.pos + amount;
	if (pos >= this.src.len || this.src.data == nil) {
		ret '\0';
	}

	ret @(this.src.data + pos);
}

fun (this: *Lexer) advance() char {
	val ch: char = this.current();

	if (this.pos < this.src.len) {
		this.pos = this.pos + 1;

		if (ch == '\n') {
			this.row   = this.row + 1;
			this.col = 1;
		}
		or {
			this.col = this.col + 1;
		}
	}

	ret ch;
}

fun (this: *Lexer) skip_whitespace() {
	for (this.at_end() == false) {
		if (ascii.is_space(this.current()) == false) {
			brk;
		}

		this.advance();
	}
}

fun (this: *Lexer) finish_token(kind: TokenKind, start: usize, row: usize, col: usize) Token {
	var token: Token;
	token.kind      = kind;
	token.module_id = this.module_id;
	token.pos       = start;
	token.len       = this.pos - start;
	token.row       = row;
	token.col       = col;
	
	ret token;
}

fun (this: *Lexer) scan_comment() Token {
	val start: usize = this.pos;
	val row:   usize = this.row;
	val col:   usize = this.col;

	# consume '#'
	this.advance();

	# iterate until end of row
	for ((this.at_end() == false) && (this.current() != '\n')) {
		this.advance();
	}

	ret this.finish_token(TOKEN_COMMENT, start, row, col);
}

fun (this: *Lexer) scan_digits(base: usize) DigitScanResult {
	var result: DigitScanResult;
	result.consumed = false;
	result.invalid  = false;

	var last_was_underscore: bool = false;

	for (this.at_end() == false) {
		val ch: char = this.current();

		if (ch == '_') {
			if (result.consumed == false || last_was_underscore) {
				result.invalid = true;
				this.advance();
				cnt;
			}

			last_was_underscore = true;
			this.advance();
			cnt;
		}

		val digit: i32 = ascii.digit_val(ch);
		if (digit < 0 || (digit :: usize) >= base) {
			result.invalid = last_was_underscore;
			brk;
		}

		result.consumed = true;
		last_was_underscore = false;
		this.advance();
	}

	if (last_was_underscore) {
		result.invalid = true;
	}

	ret result;
}

fun (this: *Lexer) scan_number() Token {
	val start: usize = this.pos;
	val row:   usize = this.row;
	val col:   usize = this.col;

	var base:    usize     = 10;
	var kind:    TokenKind = TOKEN_LIT_INT;
	var invalid: bool      = false;

	if (this.peek(0) == '0') {
		val next: char = ascii.to_lower(this.peek(1));
		if (next == 'x' || next == 'b' || next == 'o') {
			if (next == 'x') { base = 16; }
			or (next == 'b') { base = 2; }
			or (next == 'o') { base = 8; }

			this.advance(); # '0'
			this.advance(); # prefix char

			val digits: DigitScanResult = this.scan_digits(base);
			if (digits.consumed == false || digits.invalid) {
				invalid = true;
			}

			var final_kind: TokenKind = TOKEN_ERROR;
			if (invalid == false) {
				final_kind = kind;
			}

			ret this.finish_token(final_kind, start, row, col);
		}
	}

	val int_digits: DigitScanResult = this.scan_digits(base);
	if (int_digits.invalid) {
		invalid = true;
	}

	if (this.current() == '.' && this.peek(1) != '.') {
		kind = TOKEN_LIT_FLOAT;
		this.advance();

		val frac_digits: DigitScanResult = this.scan_digits(10);
		if (frac_digits.invalid) {
			invalid = true;
		}

		if (int_digits.consumed == false && frac_digits.consumed == false) {
			invalid = true;
		}
	}
	or (int_digits.consumed == false) {
		invalid = true;
	}

	var final_kind: TokenKind = TOKEN_ERROR;
	if (invalid == false) {
		final_kind = kind;
	}

	ret this.finish_token(final_kind, start, row, col);
}

fun (this: *Lexer) scan_identifier() Token {
	val start: usize = this.pos;
	val row:   usize = this.row;
	val col:   usize = this.col;

	this.advance();

	for ((this.at_end() == false) && is_identifier_continue(this.current())) {
		this.advance();
	}

	# bounds check
	if (this.src.data == nil || start >= this.src.len || this.pos > this.src.len || this.pos < start) {
		ret this.finish_token(TOKEN_ERROR, start, row, col);
	}

	var text: str;
	text.data = this.src.data + start;
	text.len  = this.pos - start;

	var kind: TokenKind = token_kind_from_identifier(text);

	ret this.finish_token(kind, start, row, col);
}

fun (this: *Lexer) scan_char_literal() Token {
	val start: usize = this.pos;
	val row:   usize = this.row;
	val col:   usize = this.col;

	var valid: bool = true;

	this.advance(); # opening quote

	if (this.at_end()) {
		valid = false;
	}
	or {
		val ch: char = this.current();

		if (ch == '\\') {
			this.advance();
			if (this.at_end()) {
				valid = false;
			}
			or (is_valid_escape(this.current()) == false) {
				valid = false;
				this.advance();
			}
			or {
				this.advance();
			}
		}
		or (ch == '\n' || ch == '\'') {
			valid = false;
		}
		or {
			this.advance();
			if (this.current() != '\'') {
				valid = false;
			}
		}

		if (valid && (this.at_end() || this.current() != '\'')) {
			valid = false;
		}

		if (this.at_end() == false && this.current() == '\'') {
			this.advance();
		}
		or {
			for ((this.at_end() == false) && (this.current() != '\n') && (this.current() != '\'')) {
				this.advance();
			}
			if (this.current() == '\'') {
				this.advance();
			}
		}
	}

	val kind: TokenKind = TOKEN_ERROR;
	if (valid) {
		kind = TOKEN_LIT_CHAR;
	}

	ret this.finish_token(kind, start, row, col);
}

fun (this: *Lexer) scan_string_literal() Token {
	val start: usize = this.pos;
	val row:   usize = this.row;
	val col:   usize = this.col;

	var valid:  bool = true;
	var closed: bool = false;

	this.advance(); # opening quote

	for (this.at_end() == false) {
		val ch: char = this.current();

		if (ch == '"') {
			this.advance();
			closed = true;
			brk;
		}

		if (ch == '\n') {
			valid = false;
			brk;
		}

		if (ch == '\\') {
			this.advance();
			if (this.at_end()) {
				valid = false;
				brk;
			}

			if (is_valid_escape(this.current()) == false) {
				valid = false;
			}

			this.advance();
			cnt;
		}

		this.advance();
	}

	if (closed == false) {
		valid = false;
	}

	if (valid == false) {
		for ((this.at_end() == false) && (this.current() != '\n') && (this.current() != '"')) {
			this.advance();
		}
		if (this.current() == '"') {
			this.advance();
		}
	}

	val kind: TokenKind = TOKEN_ERROR;
	if (valid) {
		kind = TOKEN_LIT_STRING;
	}

	ret this.finish_token(kind, start, row, col);
}

fun (this: *Lexer) scan_operator_or_punct() Token {
	val start: usize = this.pos;
	val row:   usize = this.row;
	val col:   usize = this.col;

	val ch: char = this.current();

	if (ch == '#') {
		ret this.scan_comment();
	}

	if (ch == '(') {
		this.advance();
		ret this.finish_token(TOKEN_L_PAREN, start, row, col);
	}
	if (ch == ')') {
		this.advance();
		ret this.finish_token(TOKEN_R_PAREN, start, row, col);
	}
	if (ch == '[') {
		this.advance();
		ret this.finish_token(TOKEN_L_BRACKET, start, row, col);
	}
	if (ch == ']') {
		this.advance();
		ret this.finish_token(TOKEN_R_BRACKET, start, row, col);
	}
	if (ch == '{') {
		this.advance();
		ret this.finish_token(TOKEN_L_BRACE, start, row, col);
	}
	if (ch == '}') {
		this.advance();
		ret this.finish_token(TOKEN_R_BRACE, start, row, col);
	}
	if (ch == ':') {
		this.advance();
		if (this.current() == ':') {
			this.advance();
			ret this.finish_token(TOKEN_COLON_COLON, start, row, col);
		}
		ret this.finish_token(TOKEN_COLON, start, row, col);
	}
	if (ch == ';') {
		this.advance();
		ret this.finish_token(TOKEN_SEMICOLON, start, row, col);
	}
	if (ch == '?') {
		this.advance();
		ret this.finish_token(TOKEN_QUESTION, start, row, col);
	}
	if (ch == '@') {
		this.advance();
		ret this.finish_token(TOKEN_AT, start, row, col);
	}
	if (ch == '$') {
		this.advance();
		ret this.finish_token(TOKEN_DOLLAR, start, row, col);
	}
	if (ch == '.') {
		if (this.peek(1) == '.' && this.peek(2) == '.') {
			this.advance();
			this.advance();
			this.advance();
			ret this.finish_token(TOKEN_ELLIPSIS, start, row, col);
		}
		this.advance();
		ret this.finish_token(TOKEN_DOT, start, row, col);
	}
	if (ch == ',') {
		this.advance();
		ret this.finish_token(TOKEN_COMMA, start, row, col);
	}
	if (ch == '+') {
		this.advance();
		ret this.finish_token(TOKEN_PLUS, start, row, col);
	}
	if (ch == '-') {
		this.advance();
		ret this.finish_token(TOKEN_MINUS, start, row, col);
	}
	if (ch == '*') {
		this.advance();
		ret this.finish_token(TOKEN_STAR, start, row, col);
	}
	if (ch == '%') {
		this.advance();
		ret this.finish_token(TOKEN_PERCENT, start, row, col);
	}
	if (ch == '^') {
		this.advance();
		ret this.finish_token(TOKEN_CARET, start, row, col);
	}
	if (ch == '&') {
		this.advance();
		if (this.current() == '&') {
			this.advance();
			ret this.finish_token(TOKEN_AMPERSAND_AMPERSAND, start, row, col);
		}
		ret this.finish_token(TOKEN_AMPERSAND, start, row, col);
	}
	if (ch == '|') {
		this.advance();
		if (this.current() == '|') {
			this.advance();
			ret this.finish_token(TOKEN_PIPE_PIPE, start, row, col);
		}
		ret this.finish_token(TOKEN_PIPE, start, row, col);
	}
	if (ch == '~') {
		this.advance();
		ret this.finish_token(TOKEN_TILDE, start, row, col);
	}
	if (ch == '<') {
		this.advance();
		if (this.current() == '<') {
			this.advance();
			ret this.finish_token(TOKEN_LESS_LESS, start, row, col);
		}
		if (this.current() == '=') {
			this.advance();
			ret this.finish_token(TOKEN_LESS_EQUAL, start, row, col);
		}
		ret this.finish_token(TOKEN_LESS, start, row, col);
	}
	if (ch == '>') {
		this.advance();
		if (this.current() == '>') {
			this.advance();
			ret this.finish_token(TOKEN_GREATER_GREATER, start, row, col);
		}
		if (this.current() == '=') {
			this.advance();
			ret this.finish_token(TOKEN_GREATER_EQUAL, start, row, col);
		}
		ret this.finish_token(TOKEN_GREATER, start, row, col);
	}
	if (ch == '=') {
		this.advance();
		if (this.current() == '=') {
			this.advance();
			ret this.finish_token(TOKEN_EQUAL_EQUAL, start, row, col);
		}
		ret this.finish_token(TOKEN_EQUAL, start, row, col);
	}
	if (ch == '!') {
		this.advance();
		if (this.current() == '=') {
			this.advance();
			ret this.finish_token(TOKEN_BANG_EQUAL, start, row, col);
		}
		ret this.finish_token(TOKEN_BANG, start, row, col);
	}
	if (ch == '/' || ch == '\\') {
		this.advance();
		ret this.finish_token(TOKEN_SLASH, start, row, col);
	}

	this.advance();
	ret this.finish_token(TOKEN_ERROR, start, row, col);
}

pub fun (this: *Lexer) next_token() Token {
	this.skip_whitespace();

	if (this.at_end()) {
		ret make_eof_token(this.module_id, this.pos, this.row, this.col);
	}

	val ch: char = this.current();

	if (ch == '#') {
		ret this.scan_comment();
	}

	if (ch == '\'') {
		ret this.scan_char_literal();
	}

	if (ch == '"') {
		ret this.scan_string_literal();
	}

	if (ascii.is_digit(ch) != 0) {
		ret this.scan_number();
	}

	if (is_identifier_start(ch)) {
		ret this.scan_identifier();
	}

	ret this.scan_operator_or_punct();
}

fun is_identifier_start(ch: char) bool {
	if (ch == '_') {
		ret true;
	}

	ret ascii.is_alpha(ch);
}

fun is_identifier_continue(ch: char) bool {
	if (ch == '_') {
		ret true;
	}

	ret ascii.is_alnum(ch);
}

fun token_kind_from_identifier(text: str) TokenKind {
	if (text.len == 2) {
		if (text.equals("if")) { ret TOKEN_KW_IF; }
		if (text.equals("or")) { ret TOKEN_KW_OR; }
	}

	if (text.len == 3) {
		if (text.equals("use")) { ret TOKEN_KW_USE; }
		if (text.equals("ext")) { ret TOKEN_KW_EXT; }
		if (text.equals("def")) { ret TOKEN_KW_DEF; }
		if (text.equals("pub")) { ret TOKEN_KW_PUB; }
		if (text.equals("rec")) { ret TOKEN_KW_REC; }
		if (text.equals("uni")) { ret TOKEN_KW_UNI; }
		if (text.equals("val")) { ret TOKEN_KW_VAL; }
		if (text.equals("var")) { ret TOKEN_KW_VAR; }
		if (text.equals("fun")) { ret TOKEN_KW_FUN; }
		if (text.equals("ret")) { ret TOKEN_KW_RET; }
		if (text.equals("for")) { ret TOKEN_KW_FOR; }
		if (text.equals("cnt")) { ret TOKEN_KW_CNT; }
		if (text.equals("brk")) { ret TOKEN_KW_BRK; }
		if (text.equals("asm")) { ret TOKEN_KW_ASM; }
		if (text.equals("nil")) { ret TOKEN_KW_NIL; }
	}

	ret TOKEN_IDENTIFIER;
}

fun is_valid_escape(ch: char) bool {
	if (ch == '\'') { ret true; }
	if (ch == '"')  { ret true; }
	if (ch == '\\') { ret true; }
	if (ch == 'n')  { ret true; }
	if (ch == 't')  { ret true; }
	if (ch == 'r')  { ret true; }
	if (ch == '0')  { ret true; }
	ret false;
}

fun decode_escape(ch: char) Option[char] {
	if (ch == '\'') { ret some[char]('\''); }
	if (ch == '"')  { ret some[char]('"'); }
	if (ch == '\\') { ret some[char]('\\'); }
	if (ch == 'n')  { ret some[char](10); }
	if (ch == 't')  { ret some[char](9); }
	if (ch == 'r')  { ret some[char](13); }
	if (ch == '0')  { ret some[char](0); }
	ret none[char]();
}

fun token_lexeme(source: str, token: Token) Option[str] {
	if (source.data == nil) {
		ret none[str]();
	}

	val start: usize = token.pos;
	val end:   usize = token.pos + token.len;
	if (start >= source.len || end > source.len) {
		ret none[str]();
	}

	var view: str;
	view.data = source.data + start;
	view.len  = token.len;

	ret some[str](view);
}

pub fun parse_int_literal(source: str, token: Token) Result[u64, str] {
	if (token.kind != TOKEN_LIT_INT) {
		ret err[u64, str]("token is not an integer literal");
	}

	val lexeme_opt: Option[str] = token_lexeme(source, token);
	if (lexeme_opt.is_none()) {
		ret err[u64, str]("empty integer literal");
	}

	val lexeme: str = lexeme_opt.unwrap();
	var index:  usize = 0;
	var base:   usize = 10;

	if (lexeme.len >= 2 && @(lexeme.data) == '0') {
		val prefix: char = ascii.to_lower(@(lexeme.data + 1));
		if (prefix == 'x') { base = 16; index = 2; }
		or (prefix == 'b') { base = 2; index = 2; }
		or (prefix == 'o') { base = 8; index = 2; }
	}

	var value:  u64 = 0;
	var digits: bool = false;

	for (index < lexeme.len) {
		val ch: char = @(lexeme.data + index);
		if (ch == '_') {
			index = index + 1;
			cnt;
		}

		val digit: i32 = ascii.digit_val(ch);
		if (digit < 0 || (digit :: usize) >= base) {
			ret err[u64, str]("invalid digit in integer literal");
		}

		val d: u64 = (digit :: u64);
		val next: u64 = value * (base :: u64) + d;
		if (next < value) {
			ret err[u64, str]("integer literal overflow");
		}

		value = next;
		digits = true;
		index = index + 1;
	}

	if (digits == false) {
		ret err[u64, str]("integer literal missing digits");
	}

	ret ok[u64, str](value);
}

pub fun parse_float_literal(source: str, token: Token) Result[f64, str] {
	if (token.kind != TOKEN_LIT_FLOAT) {
		ret err[f64, str]("token is not a float literal");
	}

	val lexeme_opt: Option[str] = token_lexeme(source, token);
	if (lexeme_opt.is_none()) {
		ret err[f64, str]("empty float literal");
	}

	val lexeme:     str = lexeme_opt.unwrap();
	var index:      usize = 0;
	var value:      f64   = 0.0;
	var divisor:    f64   = 1.0;
	var seen_digit: bool  = false;
	var seen_dot:   bool  = false;

	for (index < lexeme.len) {
		val ch: char = @(lexeme.data + index);

		if (ch == '_') {
			index = index + 1;
			cnt;
		}

		if (ch == '.') {
			if (seen_dot) {
				ret err[f64, str]("multiple dots in float literal");
			}
			seen_dot = true;
			index = index + 1;
			cnt;
		}

		val digit: i32 = ascii.digit_val(ch);
		if (digit < 0 || digit > 9) {
			ret err[f64, str]("invalid digit in float literal");
		}

		seen_digit = true;

		if (seen_dot == false) {
			value = value * 10.0 + (digit :: f64);
		}
		or {
			divisor = divisor * 10.0;
			value = value + ((digit :: f64) / divisor);
		}

		index = index + 1;
	}

	if (seen_digit == false) {
		ret err[f64, str]("float literal missing digits");
	}

	ret ok[f64, str](value);
}

pub fun parse_char_literal(source: str, token: Token) Result[char, str] {
	if (token.kind != TOKEN_LIT_CHAR) {
		ret err[char, str]("token is not a char literal");
	}

	val lexeme_opt: Option[str] = token_lexeme(source, token);
	if (lexeme_opt.is_none()) {
		ret err[char, str]("invalid char literal");
	}

	val lexeme: str = lexeme_opt.unwrap();
	if (lexeme.len < 3 || lexeme.data == nil) {
		ret err[char, str]("invalid char literal");
	}

	val first: char = @(lexeme.data + 1);
	if (first != '\\') {
		if (lexeme.len != 3) {
			ret err[char, str]("invalid char literal length");
		}
		ret ok[char, str](first);
	}

	if (lexeme.len < 4) {
		ret err[char, str]("unterminated escape sequence");
	}

	val escape: char = @(lexeme.data + 2);
	val decoded: Option[char] = decode_escape(escape);
	if (decoded.is_none()) {
		ret err[char, str]("unknown escape sequence");
	}

	ret ok[char, str](decoded.unwrap());
}

pub fun parse_string_literal(source: str, token: Token) Result[String, str] {
	if (token.kind != TOKEN_LIT_STRING) {
		ret err[String, str]("token is not a string literal");
	}

	val lexeme_opt: Option[str] = token_lexeme(source, token);
	if (lexeme_opt.is_none()) {
		ret err[String, str]("invalid string literal");
	}

	val lexeme: str = lexeme_opt.unwrap();
	if (lexeme.len < 2 || lexeme.data == nil) {
		ret err[String, str]("invalid string literal");
	}

	# first pass counts decoded bytes
	var payload_len: usize = 0;
	var index:       usize = 1;
	val end:         usize = lexeme.len - 1;

	for (index < end) {
		val ch: char = @(lexeme.data + index);
		if (ch == '\\') {
			index = index + 1;
			if (index >= end) {
				ret err[String, str]("unterminated escape sequence");
			}
			val escape: Option[char] = decode_escape(@(lexeme.data + index));
			if (escape.is_none()) {
				ret err[String, str]("unknown escape sequence");
			}
			payload_len = payload_len + 1;
			index = index + 1;
			cnt;
		}

		if (ch == '\n') {
			ret err[String, str]("newline in string literal");
		}

		payload_len = payload_len + 1;
		index = index + 1;
	}

	if (payload_len == 0) {
		ret ok[String, str](str{ nil, 0 });
	}

	val alloc_res: Option[*char] = mem.allocate[char](payload_len);
	if (alloc_res.is_none()) {
		ret err[String, str]("allocation failed");
	}

	val buffer: *char = alloc_res.unwrap();

	index = 1;
	var out: usize = 0;
	for (index < end) {
		val ch: char = @(lexeme.data + index);
		if (ch == '\\') {
			index = index + 1;
			val escape: Option[char] = decode_escape(@(lexeme.data + index));
			if (escape.is_none()) {
				mem.deallocate[char](buffer, payload_len);
				ret err[String, str]("unknown escape sequence");
			}
			@(buffer + out) = escape.unwrap();
			out   = out + 1;
			index = index + 1;
			cnt;
		}

		@(buffer + out) = ch;
		out   = out + 1;
		index = index + 1;
	}

	var result: str;
	result.data = buffer;
	result.len  = payload_len;
	ret ok[String, str](result);
}
